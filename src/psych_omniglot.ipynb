{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0268f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.utils.data as data\n",
    "import h5py \n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "# from skimage import io\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize,\n",
    "                                    Grayscale,\n",
    "                                    ToTensor,\n",
    "                                    Lambda\n",
    "                                   )\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e50116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "# https://gist.github.com/Kaixhin/57901e91e5c5a8bac3eb0cbbdd3aba81\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.padding_h = tuple(\n",
    "            k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight_ih = Parameter(torch.Tensor(\n",
    "            4 * out_channels, in_channels // groups, *kernel_size))\n",
    "        self.weight_hh = Parameter(torch.Tensor(\n",
    "            4 * out_channels, out_channels // groups, *kernel_size))\n",
    "        self.weight_ch = Parameter(torch.Tensor(\n",
    "            3 * out_channels, out_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "            self.register_parameter('bias_ch', None)\n",
    "        self.register_buffer('wc_blank', torch.zeros(1, 1, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "        self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "        self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "        if self.bias_ih is not None:\n",
    "            self.bias_ih.data.uniform_(-stdv, stdv)\n",
    "            self.bias_hh.data.uniform_(-stdv, stdv)\n",
    "            self.bias_ch.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        h_0, c_0 = hx\n",
    "        wx = F.conv2d(input, self.weight_ih, self.bias_ih,\n",
    "                      self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        # Cell uses a Hadamard product instead of a convolution?\n",
    "        wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(\n",
    "            wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "\n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "\n",
    "        c_1 = f * c_0 + i * g\n",
    "        h_1 = o * F.tanh(c_1)\n",
    "        return h_1, (h_1, c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaa4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# from convlstmcell import ConvLSTMCell\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, R_channels, A_channels, output_mode='error'):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.r_channels = R_channels + (0, )  # for convenience\n",
    "        self.a_channels = A_channels\n",
    "        self.n_layers = len(R_channels)\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        default_output_modes = ['prediction', 'error']\n",
    "        assert output_mode in default_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            cell = ConvLSTMCell(2 * self.a_channels[i] + self.r_channels[i+1],                                                                             self.r_channels[i],\n",
    "                                (3, 3))\n",
    "            setattr(self, 'cell{}'.format(i), cell)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            conv = nn.Sequential(nn.Conv2d(self.r_channels[i], self.a_channels[i], 3, padding=1), nn.ReLU())\n",
    "            if i == 0:\n",
    "                conv.add_module('satlu', SatLU())\n",
    "            setattr(self, 'conv{}'.format(i), conv)\n",
    "\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            update_A = nn.Sequential(nn.Conv2d(2* self.a_channels[l], self.a_channels[l+1], (3, 3), padding=1), self.maxpool)\n",
    "            setattr(self, 'update_A{}'.format(l), update_A)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for l in range(self.n_layers):\n",
    "            cell = getattr(self, 'cell{}'.format(l))\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        R_seq = [None] * self.n_layers\n",
    "        H_seq = [None] * self.n_layers\n",
    "        E_seq = [None] * self.n_layers\n",
    "\n",
    "        w, h = input.size(-2), input.size(-1)\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            # E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h))\n",
    "            # R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h))\n",
    "            E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h)).cuda()\n",
    "            R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h)).cuda()\n",
    "            w = w//2\n",
    "            h = h//2\n",
    "        time_steps = input.size(1)\n",
    "        total_error = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            A = input[:,t]\n",
    "            # A = A.type(torch.FloatTensor)\n",
    "            A = A.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                cell = getattr(self, 'cell{}'.format(l))\n",
    "                if t == 0:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = (R, R)\n",
    "                else:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = H_seq[l]\n",
    "                if l == self.n_layers - 1:\n",
    "                    R, hx = cell(E, hx)\n",
    "                else:\n",
    "                    tmp = torch.cat((E, self.upsample(R_seq[l+1])), 1)\n",
    "                    R, hx = cell(tmp, hx)\n",
    "                R_seq[l] = R\n",
    "                H_seq[l] = hx\n",
    "\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                conv = getattr(self, 'conv{}'.format(l))\n",
    "                A_hat = conv(R_seq[l])\n",
    "                if l == 0:\n",
    "                    frame_prediction = A_hat\n",
    "                pos = F.relu(A_hat - A)\n",
    "                neg = F.relu(A - A_hat)\n",
    "                E = torch.cat([pos, neg],1)\n",
    "                E_seq[l] = E\n",
    "                if l < self.n_layers - 1:\n",
    "                    update_A = getattr(self, 'update_A{}'.format(l))\n",
    "                    A = update_A(E)\n",
    "            if self.output_mode == 'error':\n",
    "                mean_error = torch.cat([torch.mean(e.view(e.size(0), -1), 1, keepdim=True) for e in E_seq], 1)\n",
    "                # batch x n_layers\n",
    "                total_error.append(mean_error)\n",
    "\n",
    "        if self.output_mode == 'error':\n",
    "            return torch.stack(total_error, 2) # batch x n_layers x nt\n",
    "        elif self.output_mode == 'prediction':\n",
    "            return frame_prediction\n",
    "\n",
    "\n",
    "class SatLU(nn.Module):\n",
    "\n",
    "    def __init__(self, lower=0, upper=255, inplace=False):\n",
    "        super(SatLU, self).__init__()\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.hardtanh(input, self.lower, self.upper, self.inplace)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' ('\\\n",
    "            + 'min_val=' + str(self.lower) \\\n",
    "    + ', max_val=' + str(self.upper) \\\n",
    "    + inplace_str + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afe203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotReactionTimeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for omniglot + reaction time data\n",
    "    Dasaset Structure:\n",
    "    label1, label2, real_file, generated_file, reaction time\n",
    "    ...\n",
    "    args:\n",
    "    - path: string - path to dataset (should be a csv file)\n",
    "    - transforms: torchvision.transforms - transforms on the data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file, transforms=None):\n",
    "        self.raw_data = pd.read_csv(data_file)\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label1 = int(self.raw_data.iloc[idx, 0])\n",
    "        label2 = int(self.raw_data.iloc[idx, 1])\n",
    "\n",
    "        im1name = self.raw_data.iloc[idx, 2]\n",
    "        image1 = Image.open(im1name)\n",
    "        im2name = self.raw_data.iloc[idx, 3]\n",
    "        image2 = Image.open(im2name)\n",
    "       \n",
    "        \n",
    "        rt = self.raw_data.iloc[idx, 4]\n",
    "        sigma_or_accuracy = self.raw_data.iloc[idx, 5]\n",
    "        \n",
    "        # if you wanted to, you could perturb one of the images. \n",
    "        # our final experiments did not do this, though. only some of them \n",
    "        # image1 = image1.filter(ImageFilter.GaussianBlur(radius = sigma_or_accuracy))\n",
    "\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "             \n",
    "        ### MODIFICATION PATCH for KITTI sizes\n",
    "        ### - upsample x 8\n",
    "        ### - resize image to whatever        \n",
    "        \n",
    "\n",
    "        sample = {'label1': label1, 'label2': label2, 'image1': image1,\n",
    "                                            'image2': image2, 'rt': rt, 'acc': sigma_or_accuracy}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55256526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 128, 160])\n"
     ]
    }
   ],
   "source": [
    "# and then the dataloader is something else here\n",
    "# and you just use it \n",
    "# and use it and focus focus focus focus focus \n",
    "\n",
    "train_transform = Compose([\n",
    "#                 RandomCrop(32, padding=4),\n",
    "                Resize((128, 160)),\n",
    "                Grayscale(num_output_channels=3),\n",
    "                RandomHorizontalFlip(),\n",
    "                ToTensor(),\n",
    "                ])\n",
    "\n",
    "path = 'sigma_dataset.csv'\n",
    "dataset = OmniglotReactionTimeDataset(path, \n",
    "            transforms=train_transform)\n",
    "\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# NOTE: not using train_loader at all \n",
    "_ = torch.utils.data.DataLoader(dataset, batch_size=16, \n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "inputs = dataiter.next()\n",
    "print(inputs['image1'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9f736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e7f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from natsort import natsorted\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "#if not os.path.exists(DATA_DIR): os.mkdir(DATA_DIR)\n",
    "desired_im_sz = (128, 160) #match kitti\n",
    "\n",
    "\n",
    "# Create image datasets.\n",
    "# Processes images and saves them in train, val, test splits.\n",
    "\n",
    "# TODO: modify this to interface with your other stuff \n",
    "\n",
    "def process_data_into_KITTI():\n",
    "    nt=10 #number of transformations per image\n",
    "    numTransf = 10\n",
    "    step = 1 # choose obj every 10 degrees of movement\n",
    "    \n",
    "    # so combine the steps of the da/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/ztaloaders \n",
    "    # so now we need to do this part \n",
    "    \n",
    "    \n",
    "    \n",
    "#     root_obj = '/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/'\n",
    "    root_obj = '/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search/omniglot_realfake/real/11'\n",
    "    stimuli = glob.glob(os.path.join(root_obj,'*.png'))\n",
    "    \n",
    "    # or just mess up our data here by making this our makeshift dataloader\n",
    "    # that seems to be more consistent with the previous work we've been doing \n",
    "    \n",
    "    X_data = np.zeros((len(stimuli),) + (nt,) + (128, 160) + (3,), np.uint8)\n",
    "        \n",
    "    print(X_data.shape)\n",
    "    for i, objID in enumerate(stimuli): #0-4000\n",
    "#         print(objID)\n",
    "        for transID in range(0, numTransf, step): \n",
    "        \n",
    "        \n",
    "        #starts at 0, up to not including nt\n",
    "#                 print(os.path.join(root_of_objects,objID))\n",
    "#                 image=imread(os.path.join(root_of_objects,objID))\n",
    "#             image=imread(os.path.join(root_obj,objID))\n",
    "            \n",
    "    \n",
    "            #TODO: I think that a big issue here is the overall image size for this\n",
    "            image = Image.open(os.path.join(root_obj, objID))\n",
    "            image = image.resize((128, 160))\n",
    "            im_arr = np.asarray(image)\n",
    "            im_arr = np.rollaxis(im_arr, 1, 0)\n",
    "\n",
    "        \n",
    "            X_data[i, (transID//step)] = im_arr\n",
    "#             1/0\n",
    "#             print(\"checkpoint_3\")\n",
    "\n",
    "    # from the other stuff, we want the batch nt chan h w\n",
    "    X_data = np.transpose(X_data,(0,1,4,2,3)) #changing the position of numChannels\n",
    "    X_data = (X_data.astype(np.float32))/255 #normalize the image\n",
    "    \n",
    "    return X_data\n",
    "\n",
    "\n",
    "# resize and crop image\n",
    "def process_im(im, desired_sz):\n",
    "#     print('in proces')\n",
    "#     print('1im shape is', im.shape)\n",
    "    target_ds = float(desired_sz[0])/im.shape[0]\n",
    "    im = np.resize(im, (desired_sz[0], int(np.round(target_ds * im.shape[1]))))\n",
    "#     print('2im shape is', im.shape)\n",
    "    d = (im.shape[1] - desired_sz[1]) / 2\n",
    "#     print('d shape is', d)\n",
    "    im = im[:, d:d+desired_sz[1]]\n",
    "#     print('im shape is', im.shape)\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a87c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10, 128, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "X_data = process_data_into_KITTI()\n",
    "# needs to be of shape: (1200, 10, 128, 160, 3)\n",
    "# so i think you got the shape correc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaad8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(prefix, var):\n",
    "    print('-------{}----------'.format(prefix))\n",
    "    if isinstance(var, torch.autograd.variable.Variable):\n",
    "        print('Variable:')\n",
    "        print('size: ', var.data.size())\n",
    "        print('data type: ', type(var.data))\n",
    "    elif isinstance(var, torch.FloatTensor) or isinstance(var, torch.cuda.FloatTensor):\n",
    "        print('Tensor:')\n",
    "        print('size: ', var.size())\n",
    "        print('type: ', type(var))\n",
    "    else:\n",
    "        print(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fe35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no, this is the DARPA dataset that we work with\n",
    "class ReactionTimeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 json_path,\n",
    "                 transform):\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        #print(\"Json file loaded: %s\" % json_path)\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.random_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[str(idx)]\n",
    "\n",
    "        # Open the image and do normalization and augmentation\n",
    "        img = Image.open(item[\"img_path\"])\n",
    "        img = img.convert('RGB')\n",
    "        # needed this transform call\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Deal with reaction times\n",
    "        if item[\"RT\"] != None:\n",
    "            rt = item[\"RT\"]\n",
    "        else:\n",
    "            rt = 0\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"rt\": rt,\n",
    "            \"category\": item[\"category\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    rt = torch.tensor([x[\"rt\"] for x in batch])\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels, \"rt\": rt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537f12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST_KITTI(Dataset):\n",
    "    def __init__(self, data, nt):\n",
    "        self.data = data\n",
    "#         self.sourcefile = sourcefile\n",
    "#         self.X = h5py.File(self.datafile, 'r')\n",
    "#         self.sources = h5py.File(self.sourcefile, 'r')\n",
    "        self.nt = nt\n",
    "#         cur_loc = 0\n",
    "#         possible_starts = []\n",
    "\n",
    "#         my_array = self.X['data_0'][()]\n",
    "#         self.X = my_array \n",
    "\n",
    "#         sources_array = self.sources['data_0'][()]\n",
    "#         self.sources = sources_array\n",
    "\n",
    "#         while cur_loc < self.X.shape[0] - self.nt + 1:\n",
    "#             if self.sources[cur_loc] == self.sources[cur_loc + self.nt - 1]:\n",
    "#                 possible_starts.append(cur_loc)\n",
    "#                 cur_loc += self.nt\n",
    "#             else:\n",
    "#                 cur_loc += 1\n",
    "#         self.possible_starts = possible_starts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         loc = self.possible_starts[index]\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17839856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search/env/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48985b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "inputs shapes are torch.Size([4, 10, 3, 128, 160])\n",
      "preds below before any modification:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3, 128, 160])\n",
      "tensor([[[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]]], dtype=torch.uint8)\n",
      "pred shape (128, 160, 3)\n",
      "[[[4 4 4]\n",
      "  [5 5 4]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]]\n",
      "\n",
      " [[5 4 4]\n",
      "  [5 5 4]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[6 6 6]\n",
      "  [6 7 6]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [6 6 6]\n",
      "  [6 6 6]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 8 7]\n",
      "  [7 8 7]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  ...\n",
      "  [6 7 6]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]]\n",
      "[[[252 252 252]\n",
      "  [251 251 252]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]]\n",
      "\n",
      " [[251 252 252]\n",
      "  [251 251 252]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " [[250 250 250]\n",
      "  [250 249 250]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 248 249]\n",
      "  [249 248 249]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  ...\n",
      "  [250 249 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]]\n",
      "(128, 160, 3)\n",
      "inputs shapes are torch.Size([4, 10, 3, 128, 160])\n",
      "preds below before any modification:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3, 128, 160])\n",
      "tensor([[[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]]], dtype=torch.uint8)\n",
      "pred shape (128, 160, 3)\n",
      "[[[4 4 4]\n",
      "  [5 5 4]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]]\n",
      "\n",
      " [[5 4 4]\n",
      "  [5 5 4]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[6 6 6]\n",
      "  [6 7 6]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [6 6 6]\n",
      "  [6 6 6]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 8 7]\n",
      "  [7 8 7]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  ...\n",
      "  [6 7 6]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]]\n",
      "[[[252 252 252]\n",
      "  [251 251 252]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]]\n",
      "\n",
      " [[251 252 252]\n",
      "  [251 251 252]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " [[250 250 250]\n",
      "  [250 249 250]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 248 249]\n",
      "  [249 248 249]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  ...\n",
      "  [250 249 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]]\n",
      "(128, 160, 3)\n",
      "inputs shapes are torch.Size([4, 10, 3, 128, 160])\n",
      "preds below before any modification:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3, 128, 160])\n",
      "tensor([[[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]],\n",
      "\n",
      "\n",
      "        [[[4, 5, 6,  ..., 7, 7, 6],\n",
      "          [5, 5, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]],\n",
      "\n",
      "         [[4, 5, 6,  ..., 7, 7, 7],\n",
      "          [4, 5, 7,  ..., 7, 7, 7],\n",
      "          [6, 7, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 7,  ..., 6, 6, 6],\n",
      "          [7, 8, 8,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 7, 5, 5]],\n",
      "\n",
      "         [[4, 4, 6,  ..., 7, 7, 6],\n",
      "          [4, 4, 6,  ..., 7, 7, 7],\n",
      "          [6, 6, 6,  ..., 7, 7, 7],\n",
      "          ...,\n",
      "          [7, 7, 6,  ..., 6, 6, 6],\n",
      "          [7, 7, 7,  ..., 7, 5, 5],\n",
      "          [7, 7, 7,  ..., 6, 5, 5]]]], dtype=torch.uint8)\n",
      "pred shape (128, 160, 3)\n",
      "[[[4 4 4]\n",
      "  [5 5 4]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]]\n",
      "\n",
      " [[5 4 4]\n",
      "  [5 5 4]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[6 6 6]\n",
      "  [6 7 6]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [6 6 6]\n",
      "  [6 6 6]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 8 7]\n",
      "  [7 8 7]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  ...\n",
      "  [6 7 6]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]]\n",
      "[[[252 252 252]\n",
      "  [251 251 252]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]]\n",
      "\n",
      " [[251 252 252]\n",
      "  [251 251 252]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " [[250 250 250]\n",
      "  [250 249 250]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 248 249]\n",
      "  [249 248 249]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  ...\n",
      "  [250 249 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]]\n",
      "(128, 160, 3)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/525707.1.gpu/ipykernel_3732388/634335632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sanity non-starting index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred_at_2.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "# import hickle as hkl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "# from kitti_data import KITTI\n",
    "# from prednet import PredNet\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def save_image(tensor, filename, nrow=8, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    from PIL import Image\n",
    "#     im = Image.fromarray(np.rollaxis(tensor.numpy(), 0, 3))\n",
    "    im = Image.fromarray(tensor.numpy())\n",
    "    print('in save_image. im shape is', im.shape)\n",
    "    im.save(filename)\n",
    "# from scipy.misc import imshow, imsave\n",
    "\n",
    "batch_size = 4\n",
    "A_channels = (3, 48, 96, 192)\n",
    "R_channels = (3, 48, 96, 192)\n",
    "nt = 10\n",
    "\n",
    "# DATA_DIR = './kitti_data'\n",
    "\n",
    "# but we need to change this to our test data that we use in the other notebook \n",
    "\n",
    "# TESTING DATA HERE >>>>\n",
    "\n",
    "# normalize = imagenetMeans TODO\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "#             RandomResizedCrop(224),\n",
    "#             RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "#             Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# old get item required an index grab too\n",
    "\n",
    "# but why did we use this TEST data instead of our own?\n",
    "# ah the X_data is still the Omniglot data here, I think ...\n",
    "\n",
    "testdataset = TEST_KITTI(data=X_data, nt=nt) # batched kitti dataset\n",
    "test_loader = DataLoader(testdataset, batch_size=4, num_workers=8)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU.')\n",
    "\n",
    "model = PredNet(R_channels, A_channels, output_mode='prediction').to(device)\n",
    "model.load_state_dict(torch.load('kitti_training.pt'))\n",
    "\n",
    "\n",
    "# dataiter = iter(test_loader)\n",
    "# inputs = dataiter.next()\n",
    "for i, inputs in enumerate(test_loader):\n",
    "\n",
    "    # print('images are', images)\n",
    "    print('inputs shapes are', inputs.shape)\n",
    "\n",
    "    inputs = Variable(inputs.to(device))\n",
    "    origin = inputs.cpu()[:, nt-1]\n",
    "    # print('origin:')\n",
    "    # print(type(origin))\n",
    "    # print(origin.size())\n",
    "\n",
    "    # print('here')\n",
    "    # print(origin[0].size())\n",
    "    \n",
    "    # so do we mess up anything on the image before running the pred?\n",
    "\n",
    "    o_np = origin[1].numpy()\n",
    "    # print('numpy shape', o_np.shape)\n",
    "    # print('numpy type', o_np.dtype)\n",
    "    # print(o_np)\n",
    "\n",
    "    # let's see if we can move the stuff around \n",
    "    channel_op = np.moveaxis(o_np, 0, -1)\n",
    "\n",
    "    # yeah the prediction signal is just simply too weak --- conclusion\n",
    "    # so let's just move on the the next datset with this model\n",
    "    \n",
    "\n",
    "    # print('numpy shape 2', channel_op.shape)\n",
    "\n",
    "    channel_op = (channel_op * 255).astype(np.uint8)\n",
    "    # print(channel_op)\n",
    "    # print('chan op shape', channel_op.shape)\n",
    "\n",
    "    # 1/0\n",
    "\n",
    "    # im2 = (o_np[0] * 255).astype(np.uint8)\n",
    "    # print(im2)\n",
    "    # print(im2.shape)\n",
    "\n",
    "    # im = Image.fromarray((o_np * 255).astype(np.uint8))\n",
    "    im = Image.fromarray(channel_op)\n",
    "\n",
    "\n",
    "\n",
    "    # print('in save_image. im shape is', im.shape)\n",
    "    im.save('origin_sanity.png')\n",
    "\n",
    "\n",
    "    # 1/0\n",
    "\n",
    "\n",
    "\n",
    "    # origin = torchvision.utils.make_grid(origin, nrow=4)\n",
    "    # save_image(origin, 'origin.jpg')\n",
    "    pred = model(inputs)\n",
    "    pred = pred.data.cpu().byte()\n",
    "    \n",
    "    # these preds are just near-blank frames 45\n",
    "    \n",
    "    print('preds below before any modification:')\n",
    "    print(type(pred))\n",
    "    print(pred.size())\n",
    "    print(pred)\n",
    "\n",
    "\n",
    "    # pred = torchvision.utils.make_grid(pred, nrow=4)\n",
    "    p_np = pred[1].numpy()\n",
    "\n",
    "    p_np = np.moveaxis(p_np, 0, -1)\n",
    "\n",
    "    print('pred shape', p_np.shape)\n",
    "#     print('numpy type', p_np. )\n",
    "    print(p_np)\n",
    "    \n",
    "    im3 = (p_np * 255).astype(np.uint8)\n",
    "    print(im3)\n",
    "    print(im3.shape)\n",
    "    # im = Image.fromarray((o_np * 255).astype(np.uint8))\n",
    "    foo = Image.fromarray(p_np)\n",
    "\n",
    "    foo.save('pred.png')\n",
    "\n",
    "    if i == 2: # sanity non-starting index \n",
    "        foo.save('pred_at_2.png')\n",
    "        1/0\n",
    "        \n",
    "    \n",
    "# save_image(pred, 'predicted.jpg')\n",
    "\n",
    "# print('um sanity check')\n",
    "\n",
    "\n",
    "# 2/0\n",
    "\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725d83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03585302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
