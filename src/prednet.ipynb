{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b212f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Justin Dulay\n",
    "\n",
    "# prednet implementation for imagenet + rt data in pytorch/pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128b4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Callback, seed_everything\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor,\n",
    "                                    Lambda\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185087fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c11c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reaction time psychophysical loss\n",
    "def RtPsychCrossEntropyLoss(outputs, targets, psych):\n",
    "#     print('in psych loss')\n",
    "#     print(type(targets))\n",
    "#     print(type(outputs))\n",
    "#     print('the outputs are', outputs)\n",
    "\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "\n",
    "    num_examples = targets.shape[0]\n",
    "    batch_size = outputs.shape[0]\n",
    "    \n",
    "#     print('in loss', targets)\n",
    "#     new_fucks = []\n",
    "#     for elem in targets: \n",
    "# #         print('the elem is .... ', elem)\n",
    "#         elem = label2id[elem]\n",
    "# #         print('the elem is now .... ', elem)\n",
    "#         new_fucks.append(int(elem))\n",
    "    \n",
    "#     targets = np.asarray([id2label[i] for i in targets])\n",
    "#     targets = torch.as_tensor(targets)\n",
    "#     print('here', new_fucks)\n",
    "#     targets = np.asarray(new_fucks)\n",
    "    \n",
    "    # converting reaction time to penalty\n",
    "    # 10002 is close to the max penalty time seen in the data\n",
    "    for idx in range(len(psych)):   \n",
    "        psych[idx] = abs(28 - psych[idx]) \n",
    "        # seems to be in terms of 10 for now,\n",
    "        # will fix later\n",
    "\n",
    "    # adding penalty to each of the output logits \n",
    "    for i in range(len(outputs)):\n",
    "#         print('psych[i]', psych[i])\n",
    "        val = psych[i] / 30\n",
    "            \n",
    "        outputs[i] += val \n",
    "\n",
    "    outputs = _log_softmax(outputs)\n",
    "    outputs = outputs[range(batch_size), targets]\n",
    "\n",
    "    return - torch.sum(outputs) / num_examples\n",
    "\n",
    "def _softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_x = torch.sum(exp_x, dim=1, keepdim=True)\n",
    "\n",
    "    return exp_x/sum_x\n",
    "\n",
    "def _log_softmax(x):\n",
    "    return torch.log(_softmax(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09c20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    rt = torch.tensor([x[\"rt\"] for x in batch])\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels, \"rt\": rt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fafee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in ./env/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in ./env/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in ./env/lib/python3.7/site-packages (from torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.7/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./env/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 22.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8450bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionTimeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 json_path,\n",
    "                 transform):\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        #print(\"Json file loaded: %s\" % json_path)\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.random_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[str(idx)]\n",
    "\n",
    "        # Open the image and do normalization and augmentation\n",
    "        img = Image.open(item[\"img_path\"])\n",
    "        img = img.convert('RGB')\n",
    "        # needed this transform call\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Deal with reaction times\n",
    "        if item[\"RT\"] != None:\n",
    "            rt = item[\"RT\"]\n",
    "        else:\n",
    "            rt = 0\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"rt\": rt,\n",
    "            \"category\": item[\"category\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04575f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "# https://gist.github.com/Kaixhin/57901e91e5c5a8bac3eb0cbbdd3aba81\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.padding_h = tuple(\n",
    "            k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight_ih = Parameter(torch.Tensor(\n",
    "            4 * out_channels, in_channels // groups, *kernel_size))\n",
    "        self.weight_hh = Parameter(torch.Tensor(\n",
    "            4 * out_channels, out_channels // groups, *kernel_size))\n",
    "        self.weight_ch = Parameter(torch.Tensor(\n",
    "            3 * out_channels, out_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "            self.register_parameter('bias_ch', None)\n",
    "        self.register_buffer('wc_blank', torch.zeros(1, 1, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "        self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "        self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "        if self.bias_ih is not None:\n",
    "            self.bias_ih.data.uniform_(-stdv, stdv)\n",
    "            self.bias_hh.data.uniform_(-stdv, stdv)\n",
    "            self.bias_ch.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        h_0, c_0 = hx\n",
    "        wx = F.conv2d(input, self.weight_ih, self.bias_ih,\n",
    "                      self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        # Cell uses a Hadamard product instead of a convolution?\n",
    "        wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(\n",
    "            wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "\n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "\n",
    "        c_1 = f * c_0 + i * g\n",
    "        h_1 = o * F.tanh(c_1)\n",
    "        return h_1, (h_1, c_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425c0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(prefix, var):\n",
    "    print('-------{}----------'.format(prefix))\n",
    "    if isinstance(var, torch.autograd.variable.Variable):\n",
    "        print('Variable:')\n",
    "        print('size: ', var.data.size())\n",
    "        print('data type: ', type(var.data))\n",
    "    elif isinstance(var, torch.FloatTensor) or isinstance(var, torch.cuda.FloatTensor):\n",
    "        print('Tensor:')\n",
    "        print('size: ', var.size())\n",
    "        print('type: ', type(var))\n",
    "    else:\n",
    "        print(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33d38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, R_channels, A_channels, output_mode='error'):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.r_channels = R_channels + (0, )  # for convenience\n",
    "        self.a_channels = A_channels\n",
    "        self.n_layers = len(R_channels)\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        default_output_modes = ['prediction', 'error']\n",
    "        assert output_mode in default_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            cell = ConvLSTMCell(2 * self.a_channels[i] + self.r_channels[i+1],                                                                             self.r_channels[i],\n",
    "                                (3, 3))\n",
    "            setattr(self, 'cell{}'.format(i), cell)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            conv = nn.Sequential(nn.Conv2d(self.r_channels[i], self.a_channels[i], 3, padding=1), nn.ReLU())\n",
    "            if i == 0:\n",
    "                conv.add_module('satlu', SatLU())\n",
    "            setattr(self, 'conv{}'.format(i), conv)\n",
    "\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            update_A = nn.Sequential(nn.Conv2d(2* self.a_channels[l], self.a_channels[l+1], (3, 3), padding=1), self.maxpool)\n",
    "            setattr(self, 'update_A{}'.format(l), update_A)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for l in range(self.n_layers):\n",
    "            cell = getattr(self, 'cell{}'.format(l))\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        R_seq = [None] * self.n_layers\n",
    "        H_seq = [None] * self.n_layers\n",
    "        E_seq = [None] * self.n_layers\n",
    "\n",
    "        w, h = input.size(-2), input.size(-1)\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h)).cuda()\n",
    "            R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h)).cuda()\n",
    "            w = w//2\n",
    "            h = h//2\n",
    "        time_steps = input.size(1)\n",
    "        total_error = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            A = input[:,t]\n",
    "            A = A.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                cell = getattr(self, 'cell{}'.format(l))\n",
    "                if t == 0:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = (R, R)\n",
    "                else:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = H_seq[l]\n",
    "                if l == self.n_layers - 1:\n",
    "                    R, hx = cell(E, hx)\n",
    "                else:\n",
    "                    tmp = torch.cat((E, self.upsample(R_seq[l+1])), 1)\n",
    "                    R, hx = cell(tmp, hx)\n",
    "                R_seq[l] = R\n",
    "                H_seq[l] = hx\n",
    "\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                conv = getattr(self, 'conv{}'.format(l))\n",
    "                A_hat = conv(R_seq[l])\n",
    "                if l == 0:\n",
    "                    frame_prediction = A_hat\n",
    "                pos = F.relu(A_hat - A)\n",
    "                neg = F.relu(A - A_hat)\n",
    "                E = torch.cat([pos, neg],1)\n",
    "                E_seq[l] = E\n",
    "                if l < self.n_layers - 1:\n",
    "                    update_A = getattr(self, 'update_A{}'.format(l))\n",
    "                    A = update_A(E)\n",
    "            if self.output_mode == 'error':\n",
    "                mean_error = torch.cat([torch.mean(e.view(e.size(0), -1), 1, keepdim=True) for e in E_seq], 1)\n",
    "                # batch x n_layers\n",
    "                total_error.append(mean_error)\n",
    "\n",
    "        if self.output_mode == 'error':\n",
    "            return torch.stack(total_error, 2) # batch x n_layers x nt\n",
    "        elif self.output_mode == 'prediction':\n",
    "            return frame_prediction\n",
    "\n",
    "\n",
    "class SatLU(nn.Module):\n",
    "\n",
    "    def __init__(self, lower=0, upper=255, inplace=False):\n",
    "        super(SatLU, self).__init__()\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.hardtanh(input, self.lower, self.upper, self.inplace)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + 'min_val=' + str(self.lower) + ', max_val=' + str(self.upper) + inplace_str + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4aa7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "class PredNetLightningModule(pl.LightningModule):\n",
    "    def __init__(self, backbone, traindataset, valdataset, testdataset):\n",
    "        super(PredNetLightningModule, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # self.vit = torchvision.models.vit_b_32(pretrained=True) \n",
    "#         self.num_labels=336\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.fc = nn.Linear(4096, 336)\n",
    "\n",
    "#         self.classifier = nn.Linear(self.vit.config.hidden_size, 164)\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        \n",
    "        self.train_dataset = traindataset\n",
    "        self.val_dataset = valdataset\n",
    "        self.test_dataset = testdataset\n",
    "\n",
    "#     def forward(self, pixel_values):\n",
    "# #         outputs = self.fc(outputs)\n",
    "#         outputs = self.vit(pixel_values=pixel_values, return_dict=False)\n",
    "#         print('type of outputs ', outputs[0].shape)\n",
    "# #         logits = self.classifier(outputs[0])\n",
    "\n",
    "#         return outputs[0]\n",
    "#         self.layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]])).to(self.device)\n",
    "#         self.time_loss_weights = torch.ones(nt, 1)\n",
    "#         self.time_loss_weights[0] = 0\n",
    "#         self.time_loss_weights = Variable(self.time_loss_weights).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    def common_step(self, batch, batch_idx):        \n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "\n",
    "#         temp = []\n",
    "#         for elem in labels: \n",
    "#             elem = label2id[elem]\n",
    "#             temp.append(int(elem))\n",
    "    \n",
    "#         labels = np.asarray(temp)\n",
    "#         labels = torch.from_numpy(labels).to(self.device)\n",
    "\n",
    "        rts = batch['rt']\n",
    "\n",
    "#         print(\"INFO. feats: {} - labels {} - rts {} --\".format(pixel_values, labels, rts))\n",
    "#         print(\"INFO. feats: {} - labels {} - rts {} --\".format(type(pixel_values), type(labels), type(rts)))\n",
    "    \n",
    "    \n",
    "        # add new dim \n",
    "        # orig shape batch x chan x w x h\n",
    "\n",
    "        inputs = inputs[:,None,:,:,:]\n",
    "        \n",
    "#         print('original inputs shape', inputs.shape)\n",
    "        # batch x time_steps x channel x width x height\n",
    "        \n",
    "        \n",
    "#         inputs = inputs.permute(0, 1, 4, 2, 3) # batch x time_steps x channel x width x height\n",
    "        inputs = Variable(inputs.to(self.device))\n",
    "        errors = self(inputs)\n",
    "        loc_batch = errors.size(0)\n",
    "        \n",
    "#         print('errors are', errors)\n",
    "\n",
    "        \n",
    "#         print('loc_batch', loc_batch)\n",
    "\n",
    "        # mm = matrix multiplication\n",
    "        # view = -1 is a *dimension. so it's reshape, but changes the tensor\n",
    "                \n",
    "        # so op here is \n",
    "#         print('errors shape', errors.shape)\n",
    "#         print('HERE')\n",
    "#         print('the op looks like:')\n",
    "        \n",
    "#         print('sanity nt is', nt)\n",
    "        \n",
    "\n",
    "        errors = errors.to(self.device)\n",
    "#         self.time_loss_weights = self.time_loss_weights.to(self.device)\n",
    "#         self.layer_loss_weights = self.layer_loss_weights.to(self.device)\n",
    "        \n",
    "        # migtht just need error there\n",
    "        \n",
    "#         print(errors.shape, time_loss_weights.shape) \n",
    "#         print('errors view is ', errors.view(-1, 1))\n",
    "        \n",
    "#         errors = torch.mm(errors.view(-1, 1), self.time_loss_weights) # batch*n_layers x 1\n",
    "        \n",
    "        \n",
    "#         print(errors.shape, layer_loss_weights.shape) \n",
    "#         print('errors view is ', errors.view(loc_batch, -1))\n",
    "        \n",
    "#         errors = torch.mm(errors.view(loc_batch, -1), self.layer_loss_weights)\n",
    "        mean_error = torch.mean(errors)\n",
    "        \n",
    "#         print('final errors are w/o layer loss or mean', errors)\n",
    "#         print('final errors shape', errors.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         1/0\n",
    "        \n",
    "#         print(\"INFO. logits: {} - labels {} - rts {} --\".format(logits, labels, rts))\n",
    "#         print(\"INFO. ---shapes--- logits: {} - labels {} - rts {} --\".format(logits.shape, labels.shape, rts.shape))\n",
    "\n",
    "#         loss = RtPsychCrossEntropyLoss(logits, labels, rts)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "        \n",
    "#         labels_hat = torch.argmax(logits, dim=1)\n",
    "#         accuracy = self.accuracy(labels_hat, labels)\n",
    "\n",
    "        return mean_error\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mean_error = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", mean_error)\n",
    "#         self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return mean_error\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mean_error = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", mean_error, on_epoch=True)\n",
    "#         self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return mean_error\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # print('batch is in testing', batch)\n",
    "        # 1/0\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_accuracy\", accuracy, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e885c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9362b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReactionTimeDataset at 0x15195be45320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# imagenetmemas\n",
    "# normalize = imagenetMeans\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(224),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "#             normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#TODO: just split up the dataset fairly \n",
    "# maybe just train/test only\n",
    "json_data_base = '/afs/crc.nd.edu/user/j/jdulay'\n",
    "train_known_known_with_rt_path = os.path.join(json_data_base, \"train_known_known_with_rt.json\")\n",
    "valid_known_known_with_rt_path = os.path.join(json_data_base, \"valid_known_known_with_rt.json\")\n",
    "\n",
    "traindataset = ReactionTimeDataset(json_path=train_known_known_with_rt_path,\n",
    "                                        transform=train_transforms)\n",
    "\n",
    "\n",
    "valdataset = traindataset\n",
    "testdataset = ReactionTimeDataset(json_path=valid_known_known_with_rt_path,\n",
    "                                        transform=train_transforms)\n",
    "\n",
    "testdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9a9b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "#TODO might need to do this for train idk\n",
    "for i in range(len(testdataset)):\n",
    "    item = testdataset.data[str(i)]['label']\n",
    "    if item not in labels:\n",
    "        labels.append(item)\n",
    "len(labels)\n",
    "# so we use this as the classes for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d954f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone is ...\n",
    "\n",
    "# these may by kitti specific ...\n",
    "A_channels = (3, 48, 96, 192)\n",
    "R_channels = (3, 48, 96, 192)\n",
    "\n",
    "backbone = PredNet(R_channels, A_channels, output_mode='error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f8716e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdulay\u001b[0m (\u001b[33mnd_cvrl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search/wandb/run-20220623_133103-phor5kx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nd_cvrl/prednet/runs/phor5kx4\" target=\"_blank\">01_prednet</a></strong> to <a href=\"https://wandb.ai/nd_cvrl/prednet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:461: UserWarning: The flag `devices=1` will be ignored, instead the device specific number [1] will be used\n",
      "  f\"The flag `devices={devices}` will be ignored, \"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len 9257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "path = '/afa/crc.nd.edu/user/j/jdulay/.cache/'\n",
    "if os.path.isdir(path):\n",
    "    os.rmdir(path)\n",
    "    \n",
    "# wandb_logger = None\n",
    "logger_name = '01_prednet'\n",
    "wandb_logger = WandbLogger(name=logger_name, project=\"prednet\")\n",
    "\n",
    "model = PredNetLightningModule(backbone=backbone,traindataset=traindataset, valdataset=valdataset, testdataset=testdataset)\n",
    "\n",
    "print('data len', len(testdataset))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    devices=1, \n",
    "#     accelerator='gpu',\n",
    "    gpus=[1],\n",
    "#     strategy='ddp',\n",
    "#     auto_select_gpus=True, \n",
    "    logger=wandb_logger,\n",
    "#     callbacks=[metrics_callback],\n",
    "    num_sanity_val_steps=2,\n",
    "#     progress_bar_refresh_rate=1000,\n",
    "    limit_train_batches=0,\n",
    "    limit_val_batches=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd25721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | PredNet          | 8.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "2 | accuracy  | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "8.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.2 M     Total params\n",
      "32.900    Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)\n",
    "\n",
    "# torch.save(model.state_dict(), 'training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d087ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 13 13:55:20 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN Xp     Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     8W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN Xp     Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 61%   85C    P2   230W / 250W |   9448MiB / 12288MiB |     77%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN Xp     Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 57%   85C    P2   240W / 250W |   8565MiB / 12288MiB |     84%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA TITAN Xp     Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8     9W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A    299480      C   .../pytorch/1.7.0/bin/python     6801MiB |\n",
      "|    1   N/A  N/A    618698      C   ...el_search/env/bin/python3     2645MiB |\n",
      "|    2   N/A  N/A    299811      C   .../pytorch/1.7.0/bin/python     8563MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ae315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at kitti_training.pt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/506902.1.gpu/ipykernel_2888422/3791114642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kitti_training.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \"\"\"\n\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_handle_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m     def _test_impl(\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: configuring sharded model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_quantization_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# reset metrics states on non-rank 0 as all states have been accumulated on rank 0 via syncing on checkpointing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "trainer.test(model, ckpt_path='kitti_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a46366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kitti_training.pt copy_kitti_training.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
