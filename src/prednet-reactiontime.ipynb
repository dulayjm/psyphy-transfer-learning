{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bab4e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Justin Dulay\n",
    "\n",
    "# prednet implementation for imagenet + rt data in pytorch/pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "128b4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Callback, seed_everything\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor,\n",
    "                                    Lambda,\n",
    "                                    ToPILImage\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260279b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69c11c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reaction time psychophysical loss\n",
    "def RtPsychCrossEntropyLoss(outputs, targets, psych):\n",
    "#     print('in psych loss')\n",
    "#     print(type(targets))\n",
    "#     print(type(outputs))\n",
    "#     print('the outputs are', outputs)\n",
    "\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "\n",
    "    num_examples = targets.shape[0]\n",
    "    batch_size = outputs.shape[0]\n",
    "    \n",
    "#     print('in loss', targets)\n",
    "#     new_fucks = []\n",
    "#     for elem in targets: \n",
    "# #         print('the elem is .... ', elem)\n",
    "#         elem = label2id[elem]\n",
    "# #         print('the elem is now .... ', elem)\n",
    "#         new_fucks.append(int(elem))\n",
    "    \n",
    "#     targets = np.asarray([id2label[i] for i in targets])\n",
    "#     targets = torch.as_tensor(targets)\n",
    "#     print('here', new_fucks)\n",
    "#     targets = np.asarray(new_fucks)\n",
    "    \n",
    "    # converting reaction time to penalty\n",
    "    # 10002 is close to the max penalty time seen in the data\n",
    "    for idx in range(len(psych)):   \n",
    "        psych[idx] = abs(28 - psych[idx]) \n",
    "        # seems to be in terms of 10 for now,\n",
    "        # will fix later\n",
    "\n",
    "    # adding penalty to each of the output logits \n",
    "    for i in range(len(outputs)):\n",
    "#         print('psych[i]', psych[i])\n",
    "        val = psych[i] / 30\n",
    "            \n",
    "        outputs[i] += val \n",
    "\n",
    "    outputs = _log_softmax(outputs)\n",
    "    outputs = outputs[range(batch_size), targets]\n",
    "\n",
    "    return - torch.sum(outputs) / num_examples\n",
    "\n",
    "def _softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_x = torch.sum(exp_x, dim=1, keepdim=True)\n",
    "\n",
    "    return exp_x/sum_x\n",
    "\n",
    "def _log_softmax(x):\n",
    "    return torch.log(_softmax(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b09c20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    rt = torch.tensor([x[\"rt\"] for x in batch])\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels, \"rt\": rt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67fafee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in ./env/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.7/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in ./env/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./env/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in ./env/lib/python3.7/site-packages (from torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 22.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83224a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeff stuff\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.callbacks import Callback\n",
    "import gc\n",
    "\n",
    "from prednet import PredNet\n",
    "#from hacked_prednet import PredNet\n",
    "from data_utils import SequenceGenerator\n",
    "from kitti_settings import *\n",
    "import hickle as hkl\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import json\n",
    "import rdm_sim_score as rdm\n",
    "import time as time_piece\n",
    "\n",
    "class SimScoreCallback(Callback):\n",
    "    \"\"\"Compute the similarity score every n epochs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hyperparams : dict\n",
    "        The list of hyperparameter values used by this model.\n",
    "    frequency : int\n",
    "        Compute the similarity score every ``frequency`` epochs and save the\n",
    "        running list of scores to file. Default 1 (checkpoint every epoch).\n",
    "    outpath : str\n",
    "        Filepath to write the similarity scores to. Default: './sim_scores.csv'\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Similarity scores are written as a single length-``n`` vector in CSV\n",
    "    format, where ``n`` is the number of epochs that have been completed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparams, nt=10, frequency=5, outpath='sim_scores.csv'):\n",
    "        super(SimScoreCallback, self).__init__()\n",
    "        self.hyperparams = hyperparams\n",
    "        self.frequency = frequency\n",
    "        self.outpath = outpath\n",
    "        self.sim_scores = []\n",
    "        self.timings = []\n",
    "        self.X_test = hkl.load(os.path.join('stimuli_test_data.hkl'))\n",
    "        self.X_test = np.rollaxis(self.X_test, 3, 2)\n",
    "        self.X_test = np.rollaxis(self.X_test, 4, 3)\n",
    "        self.nt = nt\n",
    "        self.json = \"\"\n",
    "        self.old_session=K.get_session()\n",
    "        self.session = tf.Session('')\n",
    "        print(\"init run\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.frequency == 0:\n",
    "            self.json = self.model.to_json()\n",
    "            self.old_session=K.get_session()\n",
    "            print epoch\n",
    "            self.model.save(os.path.join(self.hyperparams['ID'],'weights_epoch_'+str(epoch)+'.hdf5'),overwrite=True)\n",
    "            self.session = tf.Session()\n",
    "            with K.tf.device('/gpu:1'):\n",
    "                config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "                        inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "                config.gpu_options.allow_growth = True\n",
    "                self.session = K.tf.Session(config=config)\n",
    "            K.set_session(self.session)\n",
    "            start_time=time_piece.time()\n",
    "            sim_score = self.sim_score()\n",
    "            sim_score = self.hacked_sim_score()\n",
    "            end_time=time_piece.time()\n",
    "            tf.get_session().close()\n",
    "            cfg = K.tf.ConfigProto()\n",
    "            cfg.gpu_options.allow_growth = True\n",
    "            self.session = K.tf.Session(config=cfg)\n",
    "            gc.collect()\n",
    "            self.model.save(os.path.join(self.hyperparams['ID'],'weights_for_sim_score.hdf5'),overwrite=True)\n",
    "            json_string = self.model.to_json()\n",
    "            with open(os.path.join(self.hyperparams['ID'],'json_for_sim_score.json'),\"w\") as f:\n",
    "                f.write(json_string)\n",
    "            print(\"model saved\")\n",
    "            print(self.model)\n",
    "            K.set_session(self.session)\n",
    "            print(self.model)\n",
    "            start_time=time_piece.time() \n",
    "            sim_score = self.sim_score()\n",
    "            end_time=time_piece.time()\n",
    "            self.timings.append(end_time-start_time)\n",
    "            self.sim_scores.append(sim_score)\n",
    "            np.savetxt(self.outpath, np.array([self.sim_scores, self.timings]))\n",
    "            print self.sim_scores\n",
    "            print self.timings\n",
    "            K.clear_session()\n",
    "            K.set_session(self.old_session)\n",
    "            self.session.close()\n",
    "            self.session.delete()\n",
    "            del session\n",
    "            gc.collect()\n",
    "            K.get_session().close()\n",
    "            K.set_session(self.old_session)\n",
    "            cfg = K.tf.ConfigProto()\n",
    "            cfg.gpu_options.allow_growth = True\n",
    "            K.set_session(K.tf.Session(config=cfg))\n",
    "            K.set_session(tf.get_default_session())\n",
    "\n",
    "    def hacked_sim_score(self):\n",
    "        #X_hat = self.model.predict(self.X_test, self.hyperparams['batch_size'])\n",
    "        human_model_sim = self.model.layers[1].get_similarity_to_92_stimuli(self.X_test, self.hyperparams['batch_size'])\n",
    "        return human_model_sim\n",
    "\n",
    "    def sim_score(self):\n",
    "        times_1_to_5 = []\n",
    "        n_plot = 92\n",
    "        #print json_string\n",
    "        #f = open(os.path.join(self.hyperparams['ID'],'json_for_sim_score.json'), 'r')\n",
    "        #json_string = f.read()\n",
    "        #f.close()\n",
    "        #print json_string\n",
    "        #print self.json\n",
    "        #train_model = model_from_json(self.json, custom_objects = {'PredNet': PredNet})        \n",
    "        #train_model.load_weights(os.path.join(self.hyperparams['ID'],'weights_for_sim_score.hdf5'))\n",
    "        #train_model.set_weights(self.model.get_weights())\n",
    "        layers = self.hyperparams['layers']\n",
    "        layer_list = []\n",
    "        #for i in range(0,layers):\n",
    "        for i in range(0,layers):\n",
    "            layer_list.append('R' + str(i))\n",
    "        #X_test = hkl.load(os.path.join('stimuli_test_data.hkl'))\n",
    "        #X_test = np.rollaxis(X_test, 3, 2)\n",
    "        #X_test = np.rollaxis(X_test, 4, 3)\n",
    "        #times_1_to_5 = []\n",
    "        for layer in layer_list:\n",
    "            old_session=K.get_session()\n",
    "            tmp = tf.Session()\n",
    "            K.set_session(tmp)\n",
    "            train_model = model_from_json(self.json, custom_objects = {'PredNet': PredNet})\n",
    "            train_model.load_weights(os.path.join(self.hyperparams['ID'],'weights_for_sim_score.hdf5'))\n",
    "            start_time=time_piece.time()\n",
    "            print(layer)\n",
    "            scores_by_time = []\n",
    "            # Create testing model (to output predictions)\n",
    "            layer_config = train_model.layers[1].get_config()\n",
    "            #layer_config = self.model.layers[1].get_config()\n",
    "            layer_config['output_mode'] = layer\n",
    "            dim_ordering = layer_config['dim_ordering']\n",
    "            #old_session=K.get_session()\n",
    "            #tmp = tf.Session()\n",
    "            #K.set_session(tmp)\n",
    "            test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)\n",
    "            input_shape = list(train_model.layers[0].batch_input_shape[1:])\n",
    "            input_shape[0] = self.nt\n",
    "            inputs = Input(shape=tuple(input_shape))\n",
    "            predictions = test_prednet(inputs)\n",
    "            #tmp_sess = tf.Session()\n",
    "            #K.set_session(tmp_sess)\n",
    "            test_model = Model(input=inputs, output=predictions)\n",
    "            model_build_time=time_piece.time()\n",
    "            print(\"model_build_time\")\n",
    "            print model_build_time-start_time\n",
    "            X_hat = test_model.predict(self.X_test, self.hyperparams['batch_size'])\n",
    "            get_layer_values_time=time_piece.time()\n",
    "            print(\"get_layer_values_time\")\n",
    "            print get_layer_values_time-start_time\n",
    "            X_hat = X_hat[:,-1,:]\n",
    "            X_hat_time = X_hat.reshape(92,10,-1)\n",
    "            X_hat_time_1_to_5 = X_hat_time[:,1:5]\n",
    "            X_hat_time_1_to_5 = X_hat_time_1_to_5.reshape(92,-1)\n",
    "            times_1_to_5.append(X_hat_time_1_to_5)\n",
    "            print(\"numpy_time\")\n",
    "            numpy_time=time_piece.time()\n",
    "            print numpy_time-start_time\n",
    "            K.set_session(old_session)\n",
    "            tmp.delete()\n",
    "            #tmp.close()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        #K.clear_session()\n",
    "        #tf.set_session(original_tf_session)\n",
    "        #K.set_session(original_tf_session)\n",
    "        #f = open(os.path.join(self.hyperparams['ID'],'json_for_sim_score.json'), 'r')\n",
    "        #json_string = f.read()\n",
    "        #f.close()\n",
    "        #self.model = model_from_json(json_string, custom_objects = {'PredNet': PredNet})\n",
    "        #self.model.load_weights(os.path.join(self.hyperparams['ID'],'weights_for_sim_score.hdf5'))\n",
    "        #layer_config = self.layers[1].get_config()\n",
    "        #K.set_learning_phase(original_learning_phase)\n",
    "        #K.set_session(tf.get_default_session())\n",
    "        #layer_config = self.model.layers[1].get_config()\n",
    "        #layer_config['output_mode'] = 'error'\n",
    "        #dim_ordering = layer_config['dim_ordering']\n",
    "        #self.model = PredNet(weights=self.model.layers[1].get_weights(), **layer_config)\n",
    "\n",
    "        time_1_to_5 = np.concatenate(times_1_to_5,axis=1)\n",
    "        #individual_rdms_dict = rdm.compare_all_human_rdms_to_model(time_1_to_5)\n",
    "        score = rdm.compare_numpy_activations_from_model_to_human(time_1_to_5)\n",
    "        rdm_time=time_piece.time()\n",
    "        print(\"rdm_time\")\n",
    "        print rdm_time-start_time\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe8450bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionTimeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 json_path,\n",
    "                 transform):\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        #print(\"Json file loaded: %s\" % json_path)\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.random_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[str(idx)]\n",
    "\n",
    "        # Open the image and do normalization and augmentation\n",
    "        img = Image.open(item[\"img_path\"])\n",
    "        img = img.convert('RGB')\n",
    "        # needed this transform call\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Deal with reaction times\n",
    "        if item[\"RT\"] != None:\n",
    "            rt = item[\"RT\"]\n",
    "        else:\n",
    "            rt = 0\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"rt\": rt,\n",
    "            \"category\": item[\"category\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b37d8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "# https://gist.github.com/Kaixhin/57901e91e5c5a8bac3eb0cbbdd3aba81\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.padding_h = tuple(\n",
    "            k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight_ih = Parameter(torch.Tensor(\n",
    "            4 * out_channels, in_channels // groups, *kernel_size))\n",
    "        self.weight_hh = Parameter(torch.Tensor(\n",
    "            4 * out_channels, out_channels // groups, *kernel_size))\n",
    "        self.weight_ch = Parameter(torch.Tensor(\n",
    "            3 * out_channels, out_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "            self.register_parameter('bias_ch', None)\n",
    "        self.register_buffer('wc_blank', torch.zeros(1, 1, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "        self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "        self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "        if self.bias_ih is not None:\n",
    "            self.bias_ih.data.uniform_(-stdv, stdv)\n",
    "            self.bias_hh.data.uniform_(-stdv, stdv)\n",
    "            self.bias_ch.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        h_0, c_0 = hx\n",
    "        wx = F.conv2d(input, self.weight_ih, self.bias_ih,\n",
    "                      self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        # Cell uses a Hadamard product instead of a convolution?\n",
    "        wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(\n",
    "            wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "\n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "\n",
    "        c_1 = f * c_0 + i * g\n",
    "        h_1 = o * F.tanh(c_1)\n",
    "        return h_1, (h_1, c_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca3248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(prefix, var):\n",
    "    print('-------{}----------'.format(prefix))\n",
    "    if isinstance(var, torch.autograd.variable.Variable):\n",
    "        print('Variable:')\n",
    "        print('size: ', var.data.size())\n",
    "        print('data type: ', type(var.data))\n",
    "    elif isinstance(var, torch.FloatTensor) or isinstance(var, torch.cuda.FloatTensor):\n",
    "        print('Tensor:')\n",
    "        print('size: ', var.size())\n",
    "        print('type: ', type(var))\n",
    "    else:\n",
    "        print(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "037dbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, R_channels, A_channels, output_mode='error'):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.r_channels = R_channels + (0, )  # for convenience\n",
    "        self.a_channels = A_channels\n",
    "        self.n_layers = len(R_channels)\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        default_output_modes = ['prediction', 'error']\n",
    "        assert output_mode in default_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            cell = ConvLSTMCell(2 * self.a_channels[i] + self.r_channels[i+1],                                                                             self.r_channels[i],\n",
    "                                (3, 3))\n",
    "            setattr(self, 'cell{}'.format(i), cell)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            conv = nn.Sequential(nn.Conv2d(self.r_channels[i], self.a_channels[i], 3, padding=1), nn.ReLU())\n",
    "            if i == 0:\n",
    "                conv.add_module('satlu', SatLU())\n",
    "            setattr(self, 'conv{}'.format(i), conv)\n",
    "\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            update_A = nn.Sequential(nn.Conv2d(2* self.a_channels[l], self.a_channels[l+1], (3, 3), padding=1), self.maxpool)\n",
    "            setattr(self, 'update_A{}'.format(l), update_A)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for l in range(self.n_layers):\n",
    "            cell = getattr(self, 'cell{}'.format(l))\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def forward(self, input, rts=None):\n",
    "\n",
    "        R_seq = [None] * self.n_layers\n",
    "        H_seq = [None] * self.n_layers\n",
    "        E_seq = [None] * self.n_layers\n",
    "\n",
    "        w, h = input.size(-2), input.size(-1)\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h)).cuda()\n",
    "            R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h)).cuda()\n",
    "            w = w//2\n",
    "            h = h//2\n",
    "        time_steps = input.size(1)\n",
    "        total_error = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            A = input[:,t]\n",
    "            A = A.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            # dummy scalar loss for now\n",
    "            if torch.mean(rts) > 8:\n",
    "                A = torch.mul(A, 0.95)\n",
    "            \n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                cell = getattr(self, 'cell{}'.format(l))\n",
    "                if t == 0:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = (R, R)\n",
    "                else:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = H_seq[l]\n",
    "                if l == self.n_layers - 1:\n",
    "                    R, hx = cell(E, hx)\n",
    "                else:\n",
    "                    tmp = torch.cat((E, self.upsample(R_seq[l+1])), 1)\n",
    "                    R, hx = cell(tmp, hx)\n",
    "                R_seq[l] = R\n",
    "                H_seq[l] = hx\n",
    "\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                conv = getattr(self, 'conv{}'.format(l))\n",
    "                A_hat = conv(R_seq[l])\n",
    "                if l == 0:\n",
    "                    frame_prediction = A_hat\n",
    "                pos = F.relu(A_hat - A)\n",
    "                neg = F.relu(A - A_hat)\n",
    "                E = torch.cat([pos, neg],1)\n",
    "                E_seq[l] = E\n",
    "                if l < self.n_layers - 1:\n",
    "                    update_A = getattr(self, 'update_A{}'.format(l))\n",
    "                    A = update_A(E)\n",
    "            if self.output_mode == 'error':\n",
    "                mean_error = torch.cat([torch.mean(e.view(e.size(0), -1), 1, keepdim=True) for e in E_seq], 1)\n",
    "                # batch x n_layers\n",
    "                total_error.append(mean_error)\n",
    "\n",
    "        if self.output_mode == 'error':\n",
    "            return torch.stack(total_error, 2) # batch x n_layers x nt\n",
    "        elif self.output_mode == 'prediction':\n",
    "            return frame_prediction\n",
    "\n",
    "\n",
    "class SatLU(nn.Module):\n",
    "\n",
    "    def __init__(self, lower=0, upper=255, inplace=False):\n",
    "        super(SatLU, self).__init__()\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.hardtanh(input, self.lower, self.upper, self.inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + 'min_val=' + str(self.lower) + ', max_val=' + str(self.upper) + inplace_str + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4aa7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PredNetLightningModule(pl.LightningModule):\n",
    "    def __init__(self, backbone, traindataset, valdataset, testdataset):\n",
    "        super(PredNetLightningModule, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # self.vit = torchvision.models.vit_b_32(pretrained=True) \n",
    "#         self.num_labels=336\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.fc = nn.Linear(4096, 336)\n",
    "\n",
    "#         self.classifier = nn.Linear(self.vit.config.hidden_size, 164)\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        \n",
    "        self.train_dataset = traindataset\n",
    "        self.val_dataset = valdataset\n",
    "        self.test_dataset = testdataset\n",
    "\n",
    "#     def forward(self, pixel_values):\n",
    "# #         outputs = self.fc(outputs)\n",
    "#         outputs = self.vit(pixel_values=pixel_values, return_dict=False)\n",
    "#         print('type of outputs ', outputs[0].shape)\n",
    "# #         logits = self.classifier(outputs[0])\n",
    "\n",
    "#         return outputs[0]\n",
    "#         self.layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]])).to(self.device)\n",
    "#         self.time_loss_weights = torch.ones(nt, 1)\n",
    "#         self.time_loss_weights[0] = 0\n",
    "#         self.time_loss_weights = Variable(self.time_loss_weights).to(self.device)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.backbone.load_state_dict(torch.load(path, map_location=self.device), strict=False)\n",
    "\n",
    "    def forward(self, x, rts):\n",
    "        return self.backbone(x, rts)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=8, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    def common_step(self, batch, batch_idx):        \n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['label']\n",
    "#         labels = labels.cpu().detach().numpy()\n",
    "\n",
    "#         temp = []\n",
    "#         for elem in labels: \n",
    "#             elem = label2id[elem]\n",
    "#             temp.append(int(elem))\n",
    "    \n",
    "#         labels = np.asarray(temp)\n",
    "#         labels = torch.from_numpy(labels).to(self.device)\n",
    "\n",
    "        rts = batch['rt']\n",
    "\n",
    "#         print(\"INFO. feats: {} - labels {} - rts {} --\".format(pixel_values, labels, rts))\n",
    "#         print(\"INFO. feats: {} - labels {} - rts {} --\".format(type(pixel_values), type(labels), type(rts)))\n",
    "    \n",
    "    \n",
    "        # add new dim \n",
    "        # orig shape batch x chan x w x h\n",
    "\n",
    "        inputs = inputs[:,None,:,:,:]\n",
    "        \n",
    "#         print('original inputs shape', inputs.shape)\n",
    "        # batch x time_steps x channel x width x height\n",
    "        \n",
    "        \n",
    "#         inputs = inputs.permute(0, 1, 4, 2, 3) # batch x time_steps x channel x width x height\n",
    "        inputs = Variable(inputs.to(self.device))\n",
    "    \n",
    "    \n",
    "        errors = self(inputs, rts)\n",
    "        loc_batch = errors.size(0)\n",
    "        \n",
    "        # we might need a differnet test step entirely \n",
    "        \n",
    "#         print('errors are', errors)\n",
    "\n",
    "        \n",
    "#         print('loc_batch', loc_batch)\n",
    "\n",
    "        # mm = matrix multiplication\n",
    "        # view = -1 is a *dimension. so it's reshape, but changes the tensor\n",
    "                \n",
    "        # so op here is \n",
    "#         print('errors shape', errors.shape)\n",
    "#         print('HERE')\n",
    "#         print('the op looks like:')\n",
    "        \n",
    "#         print('sanity nt is', nt)\n",
    "        \n",
    "\n",
    "        errors = errors.to(self.device)\n",
    "#         self.time_loss_weights = self.time_loss_weights.to(self.device)\n",
    "#         self.layer_loss_weights = self.layer_loss_weights.to(self.device)\n",
    "        \n",
    "        # migtht just need error there\n",
    "        \n",
    "#         print(errors.shape, time_loss_weights.shape) \n",
    "#         print('errors view is ', errors.view(-1, 1))\n",
    "        \n",
    "#         errors = torch.mm(errors.view(-1, 1), self.time_loss_weights) # batch*n_layers x 1\n",
    "        \n",
    "        \n",
    "#         print(errors.shape, layer_loss_weights.shape) \n",
    "#         print('errors view is ', errors.view(loc_batch, -1))\n",
    "        \n",
    "#         errors = torch.mm(errors.view(loc_batch, -1), self.layer_loss_weights)\n",
    "        mean_error = torch.mean(errors)\n",
    "        \n",
    "#         print('final errors are w/o layer loss or mean', errors)\n",
    "#         print('final errors shape', errors.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         1/0\n",
    "        \n",
    "#         print(\"INFO. logits: {} - labels {} - rts {} --\".format(logits, labels, rts))\n",
    "#         print(\"INFO. ---shapes--- logits: {} - labels {} - rts {} --\".format(logits.shape, labels.shape, rts.shape))\n",
    "\n",
    "#         loss = RtPsychCrossEntropyLoss(logits, labels, rts)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "        \n",
    "#         labels_hat = torch.argmax(logits, dim=1)\n",
    "#         accuracy = self.accuracy(labels_hat, labels)\n",
    "\n",
    "        return mean_error\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mean_error = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", mean_error)\n",
    "#         self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return mean_error\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mean_error = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", mean_error, on_epoch=True)\n",
    "#         self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return mean_error\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # print('batch is in testing', batch)\n",
    "        # 1/0\n",
    "#         print('in testing, batch is', batch)\n",
    "        \n",
    "#         pred = self.common_step(batch, batch_idx) \n",
    "#         print('pred is', pred)\n",
    "\n",
    "\n",
    "        pil = ToPILImage()\n",
    "\n",
    "        inputs = batch['pixel_values']\n",
    "        rts = batch['rt']\n",
    "        test_labels = batch['label']\n",
    "        print('test_labels are', test_labels)\n",
    "        1/0\n",
    "        \n",
    "#         sanity_img = pil(inputs[0])\n",
    "# #         sanity_img.show()\n",
    "#         sanity_img = sanity_img.save('sanityorigin.png')\n",
    "        \n",
    "        inputs = inputs[:,None,:,:,:]\n",
    "        inputs = Variable(inputs.to(self.device))\n",
    "\n",
    "        \n",
    "        pred = self(inputs,rts)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('pred  is', pred)\n",
    "\n",
    "        print('pred shape is', pred.shape)\n",
    "        print('pred type is', pred.dtype)\n",
    "        img = pil(pred[0])\n",
    "        img = img.save('pred.png')\n",
    "\n",
    "#         1/0\n",
    "\n",
    "        \n",
    "#         self.log(\"test_loss\", loss, on_epoch=True)\n",
    "#         self.log(\"test_accuracy\", accuracy, on_epoch=True)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0578440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f9362b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReactionTimeDataset at 0x1465a034fdd8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# imagenetmemas\n",
    "# normalize = imagenetMeans\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(224),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "#             normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#TODO: just split up the dataset fairly \n",
    "# maybe just train/test only\n",
    "json_data_base = '/afs/crc.nd.edu/user/j/jdulay'\n",
    "train_known_known_with_rt_path = os.path.join(json_data_base, \"train_known_known_with_rt.json\")\n",
    "valid_known_known_with_rt_path = os.path.join(json_data_base, \"valid_known_known_with_rt.json\")\n",
    "\n",
    "traindataset = ReactionTimeDataset(json_path=train_known_known_with_rt_path,\n",
    "                                        transform=train_transforms)\n",
    "\n",
    "\n",
    "valdataset = traindataset\n",
    "testdataset = ReactionTimeDataset(json_path=valid_known_known_with_rt_path,\n",
    "                                        transform=train_transforms)\n",
    "\n",
    "testdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install na/tsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Processes a folder of images \n",
    "'''\n",
    "\n",
    "# so we want to modify our *json* dataset to look like this ...\n",
    "# this is going to be a bit crazy \n",
    "\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import hickle as hkl\n",
    "import matplotlib\n",
    "from readline import get_endidx\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "#if not os.path.exists(DATA_DIR): os.mkdir(DATA_DIR)\n",
    "desired_im_sz = (128, 160) #match kitti\n",
    "\n",
    "\n",
    "# Create image datasets.\n",
    "# Processes images and saves them in train, val, test splits.\n",
    "def process_data():\n",
    "    nt=10 #number of transformations per image\n",
    "    numTransf = 10\n",
    "    step = 1 # choose obj every 10 degrees of movement\n",
    "    \n",
    "    # so combine the steps of the da/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/ztaloaders \n",
    "    root_obj = '/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/'\n",
    "    \n",
    "    # but it's a fucking image folder \n",
    "    \n",
    "    \n",
    "#     json_data_base = '/afs/crc.nd.edu/user/j/jdulay'\n",
    "#     train_known_known_with_rt_path = os.path.join(json_data_base, \"train_known_known_with_rt.json\")\n",
    "    \n",
    "#     with open(train_known_known_with_rt_path) as f:\n",
    "#         data = json.load(f)\n",
    "#         print(\"Json file loaded: %s\" % json_path)\n",
    "        \n",
    "\n",
    "#     jDirs = objDirs[:8000]\n",
    "    \n",
    "    \n",
    "    # so dump all the classes into a big pile\n",
    "    \n",
    "    \n",
    "    stimuli = glob.glob(os.path.join(root_obj,'*.JPEG'))\n",
    "    #testper = 1.0 #technically does nothing\n",
    "#     stimuli=natsorted(stimuli)\n",
    "    #test = objDirs[valend:]\n",
    "    \n",
    "#     with open(os.path.join(root, 'test.txt'), 'w') as f:\n",
    "#         f.write('\\n'.join(stimuli)+'\\n')\n",
    "        \n",
    "#     print(stimuli)\n",
    "    X_data = np.zeros((len(stimuli),) + (nt,) + desired_im_sz + (3,), np.uint8)\n",
    "    for i, objID in enumerate(stimuli): #0-4000\n",
    "        print(objID)\n",
    "        try:\n",
    "            for transID in range(0, numTransf, step): #starts at 0, up to not including nt\n",
    "#                 print(os.path.join(root_of_objects,objID))\n",
    "#                 image=imread(os.path.join(root_of_objects,objID))\n",
    "                \n",
    "                \n",
    "                item = data[str(transID)]\n",
    "\n",
    "                # Open the image and do normalization and augmentation\n",
    "#                 im/g = Image.open(item[\"img_path\"])\n",
    "\n",
    "                image = imread(item[\"img_path\"])\n",
    "                \n",
    "                \n",
    "                print(\"checkpoint_1\")\n",
    "                image = cv2.resize(image, (desired_im_sz[1], desired_im_sz[0]))\n",
    "                print(\"checkpoint_2\")\n",
    "                print(transID/step)\n",
    "                X_data[i, (transID/step)] = process_im(image, desired_im_sz)\n",
    "                print(\"checkpoint_3\")\n",
    "        except:\n",
    "            print(\"skipped\")\n",
    "    X_data = np.transpose(X_data,(0,1,4,2,3)) #changing the position of numChannels\n",
    "    X_data = (X_data.astype(np.float32))/255 #normalize the image\n",
    "    hkl.dump(X_data, os.path.join(root,'stimuli_test_data.hkl'))\n",
    "\n",
    "\n",
    "# resize and crop image\n",
    "def process_im(im, desired_sz):\n",
    "    target_ds = float(desired_sz[0])/im.shape[0]\n",
    "    im = imresize(im, (desired_sz[0], int(np.round(target_ds * im.shape[1]))))\n",
    "    d = (im.shape[1] - desired_sz[1]) / 2\n",
    "    im = im[:, d:d+desired_sz[1]]\n",
    "    return im\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e9a9b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "#TODO might need to do this for train idk\n",
    "for i in range(len(testdataset)):\n",
    "    item = testdataset.data[str(i)]['label']\n",
    "    if item not in labels:\n",
    "        labels.append(item)\n",
    "len(labels)\n",
    "# so we use this as the classes for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d954f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone is ...\n",
    "\n",
    "# these may by kitti specific ...\n",
    "A_channels = (3, 48, 96, 192)\n",
    "R_channels = (3, 48, 96, 192)\n",
    "\n",
    "backbone = PredNet(R_channels, A_channels, output_mode='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47f8716e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len 9257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "path = '/afa/crc.nd.edu/user/j/jdulay/.cache/'\n",
    "if os.path.isdir(path):\n",
    "    os.rmdir(path)\n",
    "    \n",
    "# wandb_logger = None\n",
    "logger_name = '01_prednet'\n",
    "# wandb_logger = WandbLogger(name=logger_name, project=\"prednet\")\n",
    "wandb_logger = None\n",
    "\n",
    "model = PredNetLightningModule(backbone=backbone,traindataset=traindataset, valdataset=valdataset, testdataset=testdataset)\n",
    "\n",
    "print('data len', len(testdataset))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    devices=1, \n",
    "#     accelerator='gpu',\n",
    "    gpus=[1],\n",
    "#     strategy='ddp',\n",
    "#     auto_select_gpus=True, \n",
    "#     logger=wandb_logger,\n",
    "#     callbacks=[metrics_callback],\n",
    "    num_sanity_val_steps=2,\n",
    "#     progress_bar_refresh_rate=1000,\n",
    "#     limit_train_batches=0,\n",
    "#     limit_val_batches=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fd25721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer.fit(model)\n",
    "\n",
    "# torch.save(model.state_dict(), 'A_rt_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a812d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, ckpt_path='training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48cba2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.cell0.weight_ih\n",
      "backbone.cell0.weight_hh\n",
      "backbone.cell0.weight_ch\n",
      "backbone.cell0.bias_ih\n",
      "backbone.cell0.bias_hh\n",
      "backbone.cell0.bias_ch\n",
      "backbone.cell0.wc_blank\n",
      "backbone.cell1.weight_ih\n",
      "backbone.cell1.weight_hh\n",
      "backbone.cell1.weight_ch\n",
      "backbone.cell1.bias_ih\n",
      "backbone.cell1.bias_hh\n",
      "backbone.cell1.bias_ch\n",
      "backbone.cell1.wc_blank\n",
      "backbone.cell2.weight_ih\n",
      "backbone.cell2.weight_hh\n",
      "backbone.cell2.weight_ch\n",
      "backbone.cell2.bias_ih\n",
      "backbone.cell2.bias_hh\n",
      "backbone.cell2.bias_ch\n",
      "backbone.cell2.wc_blank\n",
      "backbone.cell3.weight_ih\n",
      "backbone.cell3.weight_hh\n",
      "backbone.cell3.weight_ch\n",
      "backbone.cell3.bias_ih\n",
      "backbone.cell3.bias_hh\n",
      "backbone.cell3.bias_ch\n",
      "backbone.cell3.wc_blank\n",
      "backbone.conv0.0.weight\n",
      "backbone.conv0.0.bias\n",
      "backbone.conv1.0.weight\n",
      "backbone.conv1.0.bias\n",
      "backbone.conv2.0.weight\n",
      "backbone.conv2.0.bias\n",
      "backbone.conv3.0.weight\n",
      "backbone.conv3.0.bias\n",
      "backbone.update_A0.0.weight\n",
      "backbone.update_A0.0.bias\n",
      "backbone.update_A1.0.weight\n",
      "backbone.update_A1.0.bias\n",
      "backbone.update_A2.0.weight\n",
      "backbone.update_A2.0.bias\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "new_checkpoint = OrderedDict()\n",
    "\n",
    "checkpoint=torch.load('training.pt')\n",
    "\n",
    "\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa3a6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_model('trainer_02.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9818d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.output_mode='prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b970e107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at trainer_02.pt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at trainer_02.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941429965e564ab9b3824168022b0ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_labels are tensor([267, 280, 292, 270, 273, 260, 292, 288], device='cuda:1')\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/495109.1.gpu/ipykernel_3700439/939913629.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'trainer_02.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \"\"\"\n\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_handle_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m     def _test_impl(\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"run_{self.state.stage}_evaluation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m             \u001b[0meval_loop_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0;31m# remove the tensors from the eval results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/generic_model_search/env/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/495109.1.gpu/ipykernel_3700439/3654607737.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_labels are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;31m#         sanity_img = pil(inputs[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "trainer.test(model, ckpt_path='trainer_02.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0ce5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
