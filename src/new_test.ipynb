{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0268f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.utils.data as data\n",
    "import h5py \n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor,\n",
    "                                    Lambda\n",
    "                                   )\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class KITTI(Dataset):\n",
    "    # yeah the sources are the labels. \n",
    "    def __init__(self, datafile, sourcefile, nt):\n",
    "        self.datafile = datafile\n",
    "        self.sourcefile = sourcefile\n",
    "        self.X = h5py.File(self.datafile, 'r')\n",
    "        self.sources = h5py.File(self.sourcefile, 'r')\n",
    "        self.nt = nt\n",
    "        cur_loc = 0\n",
    "        possible_starts = []\n",
    "\n",
    "        my_array = self.X['data_0'][()]\n",
    "        self.X = my_array \n",
    "\n",
    "        sources_array = self.sources['data_0'][()]\n",
    "        self.sources = sources_array\n",
    "\n",
    "        while cur_loc < self.X.shape[0] - self.nt + 1:\n",
    "            if self.sources[cur_loc] == self.sources[cur_loc + self.nt - 1]:\n",
    "                possible_starts.append(cur_loc)\n",
    "                cur_loc += self.nt\n",
    "            else:\n",
    "                cur_loc += 1\n",
    "        self.possible_starts = possible_starts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        loc = self.possible_starts[index]\n",
    "        return self.X[loc:loc+self.nt]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.possible_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afe203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST_KITTI(Dataset):\n",
    "    def __init__(self, data, nt):\n",
    "        self.data = data\n",
    "#         self.sourcefile = sourcefile\n",
    "#         self.X = h5py.File(self.datafile, 'r')\n",
    "#         self.sources = h5py.File(self.sourcefile, 'r')\n",
    "        self.nt = nt\n",
    "#         cur_loc = 0\n",
    "#         possible_starts = []\n",
    "\n",
    "#         my_array = self.X['data_0'][()]\n",
    "#         self.X = my_array \n",
    "\n",
    "#         sources_array = self.sources['data_0'][()]\n",
    "#         self.sources = sources_array\n",
    "\n",
    "#         while cur_loc < self.X.shape[0] - self.nt + 1:\n",
    "#             if self.sources[cur_loc] == self.sources[cur_loc + self.nt - 1]:\n",
    "#                 possible_starts.append(cur_loc)\n",
    "#                 cur_loc += self.nt\n",
    "#             else:\n",
    "#                 cur_loc += 1\n",
    "#         self.possible_starts = possible_starts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         loc = self.possible_starts[index]\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e50116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "# https://gist.github.com/Kaixhin/57901e91e5c5a8bac3eb0cbbdd3aba81\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.padding_h = tuple(\n",
    "            k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight_ih = Parameter(torch.Tensor(\n",
    "            4 * out_channels, in_channels // groups, *kernel_size))\n",
    "        self.weight_hh = Parameter(torch.Tensor(\n",
    "            4 * out_channels, out_channels // groups, *kernel_size))\n",
    "        self.weight_ch = Parameter(torch.Tensor(\n",
    "            3 * out_channels, out_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n",
    "            self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "            self.register_parameter('bias_ch', None)\n",
    "        self.register_buffer('wc_blank', torch.zeros(1, 1, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "        self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "        self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "        if self.bias_ih is not None:\n",
    "            self.bias_ih.data.uniform_(-stdv, stdv)\n",
    "            self.bias_hh.data.uniform_(-stdv, stdv)\n",
    "            self.bias_ch.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        h_0, c_0 = hx\n",
    "        wx = F.conv2d(input, self.weight_ih, self.bias_ih,\n",
    "                      self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        # Cell uses a Hadamard product instead of a convolution?\n",
    "        wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride,\n",
    "                      self.padding_h, self.dilation, self.groups)\n",
    "\n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(\n",
    "            wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "\n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "\n",
    "        c_1 = f * c_0 + i * g\n",
    "        h_1 = o * F.tanh(c_1)\n",
    "        return h_1, (h_1, c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e7f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from natsort import natsorted\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "#if not os.path.exists(DATA_DIR): os.mkdir(DATA_DIR)\n",
    "desired_im_sz = (128, 160) #match kitti\n",
    "\n",
    "\n",
    "# Create image datasets.\n",
    "# Processes images and saves them in train, val, test splits.\n",
    "def process_data():\n",
    "    nt=10 #number of transformations per image\n",
    "    numTransf = 10\n",
    "    step = 1 # choose obj every 10 degrees of movement\n",
    "    \n",
    "    # so combine the steps of the da/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/ztaloaders \n",
    "    root_obj = '/scratch365/jhuang24/dataset_v1_3_partition/train_valid/known_known/00403/'\n",
    "    \n",
    "    \n",
    "    \n",
    "#     json_data_base = '/afs/crc.nd.edu/user/j/jdulay'\n",
    "#     train_known_known_with_rt_path = os.path.join(json_data_base, \"train_known_known_with_rt.json\")\n",
    "    \n",
    "#     with open(train_known_known_with_rt_path) as f:\n",
    "#         data = json.load(f)\n",
    "#         print(\"Json file loaded: %s\" % json_path)\n",
    "        \n",
    "\n",
    "#     jDirs = objDirs[:8000]\n",
    "    \n",
    "    \n",
    "    # so dump all the classes into a big pile\n",
    "    \n",
    "    \n",
    "    stimuli = glob.glob(os.path.join(root_obj,'*.JPEG'))\n",
    "    #testper = 1.0 #technically does nothing\n",
    "#     stimuli=natsorted(stimuli)\n",
    "    #test = objDirs[valend:]\n",
    "    \n",
    "#     with open(os.path.join(root, 'test.txt'), 'w') as f:\n",
    "#         f.write('\\n'.join(stimuli)+'\\n')\n",
    "        \n",
    "#     print(stimuli)\n",
    "    X_data = np.zeros((len(stimuli),) + (nt,) + (128, 160) + (3,), np.uint8)\n",
    "    print(X_data.shape)\n",
    "    for i, objID in enumerate(stimuli): #0-4000\n",
    "#         print(objID)\n",
    "        for transID in range(0, numTransf, step): #starts at 0, up to not including nt\n",
    "#                 print(os.path.join(root_of_objects,objID))\n",
    "#                 image=imread(os.path.join(root_of_objects,objID))\n",
    "#             image=imread(os.path.join(root_obj,objID))\n",
    "            \n",
    "            image = Image.open(os.path.join(root_obj, objID))\n",
    "            image = image.resize((128, 160))\n",
    "            im_arr = np.asarray(image)\n",
    "            im_arr = np.rollaxis(im_arr, 1, 0)\n",
    "            \n",
    "#             sanity_pill = Image.fromarray(im_arr)\n",
    "#             sanity_pill.save('sanity2.jpg')\n",
    "#             print('im_arr shape', im_arr.shape)\n",
    "            \n",
    "        \n",
    "        \n",
    "#             print('the path is um, ', os.path.join(root_obj,objID))\n",
    "            # we don't need the weird json loading stuff here, because of how\n",
    "            # we set it up before ... \n",
    "            #item = data[str(transID)]\n",
    "            # transID is just a num \n",
    "            #\n",
    "            \n",
    "            #image = imread(item[\"img_path\"])\n",
    "                \n",
    "#             print(\"checkpoint_1\")\n",
    "            #image = cv2.resize(image, (desired_im_sz[1], desired_im_sz[0]))\n",
    "#             print('pre image shape is,', image.shape)\n",
    "\n",
    "\n",
    "\n",
    "#             image = np.resize(image, (desired_im_sz[0], desired_im_sz[1], 3))\n",
    "\n",
    "    \n",
    "    \n",
    "#             print('pre image shape is,', image.shape)\n",
    "#             print(\"checkpoint_2\")\n",
    "#             print(transID/step)\n",
    "\n",
    "#             print('shape is ', image.shape)\n",
    "            # X_data[i, (transID/step)] = process_im(image, desired_im_sz)\n",
    "#             print(transID/step)\n",
    "#             print(i)\n",
    "#             print(X_data.shape)\n",
    "#             print('type of the image going in is', type(image))\n",
    "            #print('eek', X_data[0,(transID/step)])\n",
    "            #X_data[i, (transID/step)] = process_im(image, desired_im_sz[1])\n",
    "#             print('type of image is', type(image))\n",
    "#             print('image shape is', image.shape)\n",
    "            \n",
    "#             print('image shape is', np.rollaxis(image, 0, 3).shape)\n",
    "\n",
    "#             im = np.rollaxis(image, 2, 0)\n",
    "#             im = Image.fromarray(np.rollaxis(image, 2, 0))\n",
    "#             im = Image.fromarray(image)\n",
    "\n",
    "#             image.save('sanity_in_process.jpg')\n",
    "        \n",
    "            X_data[i, (transID//step)] = im_arr\n",
    "#             1/0\n",
    "#             print(\"checkpoint_3\")\n",
    "\n",
    "    # from the other stuff, we want the batch nt chan h w\n",
    "    X_data = np.transpose(X_data,(0,1,4,2,3)) #changing the position of numChannels\n",
    "    X_data = (X_data.astype(np.float32))/255 #normalize the image\n",
    "    \n",
    "    return X_data\n",
    "\n",
    "\n",
    "# resize and crop image\n",
    "def process_im(im, desired_sz):\n",
    "#     print('in proces')\n",
    "#     print('1im shape is', im.shape)\n",
    "    target_ds = float(desired_sz[0])/im.shape[0]\n",
    "    im = np.resize(im, (desired_sz[0], int(np.round(target_ds * im.shape[1]))))\n",
    "#     print('2im shape is', im.shape)\n",
    "    d = (im.shape[1] - desired_sz[1]) / 2\n",
    "#     print('d shape is', d)\n",
    "    im = im[:, d:d+desired_sz[1]]\n",
    "#     print('im shape is', im.shape)\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a87c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 10, 128, 160, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/524941.1.gpu/ipykernel_3596198/3903243580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/524941.1.gpu/ipykernel_3596198/1658482588.py\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#             image=imread(os.path.join(root_obj,objID))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mim_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "X_data = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(prefix, var):\n",
    "    print('-------{}----------'.format(prefix))\n",
    "    if isinstance(var, torch.autograd.variable.Variable):\n",
    "        print('Variable:')\n",
    "        print('size: ', var.data.size())\n",
    "        print('data type: ', type(var.data))\n",
    "    elif isinstance(var, torch.FloatTensor) or isinstance(var, torch.cuda.FloatTensor):\n",
    "        print('Tensor:')\n",
    "        print('size: ', var.size())\n",
    "        print('type: ', type(var))\n",
    "    else:\n",
    "        print(type(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# from convlstmcell import ConvLSTMCell\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, R_channels, A_channels, output_mode='error'):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.r_channels = R_channels + (0, )  # for convenience\n",
    "        self.a_channels = A_channels\n",
    "        self.n_layers = len(R_channels)\n",
    "        self.output_mode = output_mode\n",
    "\n",
    "        default_output_modes = ['prediction', 'error']\n",
    "        assert output_mode in default_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            cell = ConvLSTMCell(2 * self.a_channels[i] + self.r_channels[i+1],                                                                             self.r_channels[i],\n",
    "                                (3, 3))\n",
    "            setattr(self, 'cell{}'.format(i), cell)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            conv = nn.Sequential(nn.Conv2d(self.r_channels[i], self.a_channels[i], 3, padding=1), nn.ReLU())\n",
    "            if i == 0:\n",
    "                conv.add_module('satlu', SatLU())\n",
    "            setattr(self, 'conv{}'.format(i), conv)\n",
    "\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            update_A = nn.Sequential(nn.Conv2d(2* self.a_channels[l], self.a_channels[l+1], (3, 3), padding=1), self.maxpool)\n",
    "            setattr(self, 'update_A{}'.format(l), update_A)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for l in range(self.n_layers):\n",
    "            cell = getattr(self, 'cell{}'.format(l))\n",
    "            cell.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        R_seq = [None] * self.n_layers\n",
    "        H_seq = [None] * self.n_layers\n",
    "        E_seq = [None] * self.n_layers\n",
    "\n",
    "        w, h = input.size(-2), input.size(-1)\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            # E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h))\n",
    "            # R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h))\n",
    "            E_seq[l] = Variable(torch.zeros(batch_size, 2*self.a_channels[l], w, h)).cuda()\n",
    "            R_seq[l] = Variable(torch.zeros(batch_size, self.r_channels[l], w, h)).cuda()\n",
    "            w = w//2\n",
    "            h = h//2\n",
    "        time_steps = input.size(1)\n",
    "        total_error = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            A = input[:,t]\n",
    "            # A = A.type(torch.FloatTensor)\n",
    "            A = A.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            for l in reversed(range(self.n_layers)):\n",
    "                cell = getattr(self, 'cell{}'.format(l))\n",
    "                if t == 0:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = (R, R)\n",
    "                else:\n",
    "                    E = E_seq[l]\n",
    "                    R = R_seq[l]\n",
    "                    hx = H_seq[l]\n",
    "                if l == self.n_layers - 1:\n",
    "                    R, hx = cell(E, hx)\n",
    "                else:\n",
    "                    tmp = torch.cat((E, self.upsample(R_seq[l+1])), 1)\n",
    "                    R, hx = cell(tmp, hx)\n",
    "                R_seq[l] = R\n",
    "                H_seq[l] = hx\n",
    "\n",
    "\n",
    "            for l in range(self.n_layers):\n",
    "                conv = getattr(self, 'conv{}'.format(l))\n",
    "                A_hat = conv(R_seq[l])\n",
    "                if l == 0:\n",
    "                    frame_prediction = A_hat\n",
    "                pos = F.relu(A_hat - A)\n",
    "                neg = F.relu(A - A_hat)\n",
    "                E = torch.cat([pos, neg],1)\n",
    "                E_seq[l] = E\n",
    "                if l < self.n_layers - 1:\n",
    "                    update_A = getattr(self, 'update_A{}'.format(l))\n",
    "                    A = update_A(E)\n",
    "            if self.output_mode == 'error':\n",
    "                mean_error = torch.cat([torch.mean(e.view(e.size(0), -1), 1, keepdim=True) for e in E_seq], 1)\n",
    "                # batch x n_layers\n",
    "                total_error.append(mean_error)\n",
    "\n",
    "        if self.output_mode == 'error':\n",
    "            return torch.stack(total_error, 2) # batch x n_layers x nt\n",
    "        elif self.output_mode == 'prediction':\n",
    "            return frame_prediction\n",
    "\n",
    "\n",
    "class SatLU(nn.Module):\n",
    "\n",
    "    def __init__(self, lower=0, upper=255, inplace=False):\n",
    "        super(SatLU, self).__init__()\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.hardtanh(input, self.lower, self.upper, self.inplace)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' ('\\\n",
    "            + 'min_val=' + str(self.lower) \\\n",
    "    + ', max_val=' + str(self.upper) \\\n",
    "    + inplace_str + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fe35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DARPA_ReactionTimeDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 json_path,\n",
    "                 transform):\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        #print(\"Json file loaded: %s\" % json_path)\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.random_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[str(idx)]\n",
    "\n",
    "        # Open the image and do normalization and augmentation\n",
    "        img = Image.open(item[\"img_path\"])\n",
    "        img = img.convert('RGB')\n",
    "        # needed this transform call\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Deal with reaction times\n",
    "        if item[\"RT\"] != None:\n",
    "            rt = item[\"RT\"]\n",
    "        else:\n",
    "            rt = 0\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"rt\": rt,\n",
    "            \"category\": item[\"category\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    rt = torch.tensor([x[\"rt\"] for x in batch])\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels, \"rt\": rt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17839856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/j/jdulay/research/generic_model_search/env/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "48985b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "inputs shapes are torch.Size([4, 10, 3, 128, 160])\n",
      "origin:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3, 128, 160])\n",
      "here\n",
      "torch.Size([3, 128, 160])\n",
      "numpy shape (3, 128, 160)\n",
      "numpy type float32\n",
      "[[[0.41568628 0.41568628 0.41960785 ... 0.65882355 0.7019608  0.7019608 ]\n",
      "  [0.42745098 0.43529412 0.42745098 ... 0.6509804  0.654902   0.6666667 ]\n",
      "  [0.43529412 0.45490196 0.43529412 ... 0.6627451  0.67058825 0.65882355]\n",
      "  ...\n",
      "  [0.3882353  0.3882353  0.40784314 ... 0.6        0.5764706  0.5647059 ]\n",
      "  [0.3764706  0.39607844 0.38039216 ... 0.63529414 0.5803922  0.58431375]\n",
      "  [0.37254903 0.38039216 0.3647059  ... 0.627451   0.5803922  0.58431375]]\n",
      "\n",
      " [[0.3647059  0.36862746 0.37254903 ... 0.6156863  0.654902   0.654902  ]\n",
      "  [0.38431373 0.38039216 0.38039216 ... 0.6117647  0.62352943 0.6509804 ]\n",
      "  [0.3882353  0.4117647  0.40392157 ... 0.63529414 0.6509804  0.65882355]\n",
      "  ...\n",
      "  [0.33333334 0.34117648 0.36078432 ... 0.5686275  0.5372549  0.5372549 ]\n",
      "  [0.32941177 0.34117648 0.34509805 ... 0.58431375 0.53333336 0.5372549 ]\n",
      "  [0.3137255  0.32941177 0.31764707 ... 0.5686275  0.53333336 0.5568628 ]]\n",
      "\n",
      " [[0.26666668 0.2627451  0.2627451  ... 0.5019608  0.5254902  0.5372549 ]\n",
      "  [0.2784314  0.27450982 0.28235295 ... 0.50980395 0.54509807 0.5686275 ]\n",
      "  [0.28627452 0.28627452 0.30588236 ... 0.5411765  0.5764706  0.59607846]\n",
      "  ...\n",
      "  [0.23529412 0.24313726 0.25490198 ... 0.44705883 0.42745098 0.42352942]\n",
      "  [0.23137255 0.23529412 0.23529412 ... 0.41960785 0.41960785 0.41960785]\n",
      "  [0.21176471 0.22352941 0.21568628 ... 0.40784314 0.40392157 0.41568628]]]\n",
      "numpy shape 2 (128, 160, 3)\n",
      "chan op shape (128, 160, 3)\n",
      "[[106 106 107 ... 168 179 179]\n",
      " [109 111 109 ... 166 167 170]\n",
      " [111 116 111 ... 169 171 168]\n",
      " ...\n",
      " [ 99  99 104 ... 153 147 144]\n",
      " [ 96 101  97 ... 162 148 149]\n",
      " [ 95  97  93 ... 160 148 149]]\n",
      "(128, 160)\n",
      "preds below:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3, 128, 160])\n",
      "pred shape (128, 160, 3)\n",
      "numpy type uint8\n",
      "[[[5 5 4]\n",
      "  [5 5 4]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 7 6]]\n",
      "\n",
      " [[5 5 4]\n",
      "  [5 5 5]\n",
      "  [6 7 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[6 7 6]\n",
      "  [6 7 6]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [6 6 6]\n",
      "  ...\n",
      "  [6 6 6]\n",
      "  [6 6 6]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  ...\n",
      "  [7 7 6]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]\n",
      "  ...\n",
      "  [6 7 6]\n",
      "  [5 5 5]\n",
      "  [5 5 5]]]\n",
      "[[[251 251 252]\n",
      "  [251 251 252]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 249 250]]\n",
      "\n",
      " [[251 251 252]\n",
      "  [251 251 251]\n",
      "  [250 249 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " [[250 249 250]\n",
      "  [250 249 250]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  ...\n",
      "  [249 249 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[249 249 249]\n",
      "  [249 249 249]\n",
      "  [249 249 249]\n",
      "  ...\n",
      "  [250 249 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]]\n",
      "(128, 160, 3)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/512342.1.gpu/ipykernel_3080916/2123663149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "# import hickle as hkl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "# from kitti_data import KITTI\n",
    "# from prednet import PredNet\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def save_image(tensor, filename, nrow=8, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    from PIL import Image\n",
    "#     im = Image.fromarray(np.rollaxis(tensor.numpy(), 0, 3))\n",
    "    im = Image.fromarray(tensor.numpy())\n",
    "    print('in save_image. im shape is', im.shape)\n",
    "    im.save(filename)\n",
    "# from scipy.misc import imshow, imsave\n",
    "\n",
    "batch_size = 4\n",
    "A_channels = (3, 48, 96, 192)\n",
    "R_channels = (3, 48, 96, 192)\n",
    "nt = 10\n",
    "\n",
    "# DATA_DIR = './kitti_data'\n",
    "\n",
    "# but we need to change this to our test data that we use in the other notebook \n",
    "\n",
    "# TESTING DATA HERE >>>>\n",
    "\n",
    "# normalize = imagenetMeans TODO\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(224),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# old get item required an index grab too\n",
    "testdataset = TEST_KITTI(data=X_data, nt=nt) # batched kitti dataset\n",
    "test_loader = DataLoader(testdataset, batch_size=4, num_workers=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: just split up the dataset fairly \n",
    "# maybe just train/test only\n",
    "# json_data_base = '/afs/crc.nd.edu/user/j/jdulay'\n",
    "# train_known_known_with_rt_path = os.path.join(json_data_base, \"train_known_known_with_rt.json\")\n",
    "# valid_known_known_with_rt_path = os.path.join(json_data_base, \"valid_known_known_with_rt.json\")\n",
    "\n",
    "\n",
    "# # valdataset = traindataset\n",
    "# testdataset = ReactionTimeDataset(json_path=valid_known_known_with_rt_path,\n",
    "#                                         transform=train_transforms)\n",
    "\n",
    "# labels = []\n",
    "# #TODO might need to do this for train idk\n",
    "# for i in range(len(testdataset)):\n",
    "#     item = testdataset.data[str(i)]['label']\n",
    "#     if item not in labels:\n",
    "#         labels.append(item)\n",
    "# len(labels)\n",
    "\n",
    "# test_loader = DataLoader(testdataset, batch_size=16, num_workers=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# >>>>>>>>>\n",
    "\n",
    "\n",
    "\n",
    "# DATA_DIR = '/media/lei/000F426D0004CCF4/datasets/kitti_data'\n",
    "# test_file = os.path.join(DATA_DIR, 'X_test.hkl')\n",
    "# test_sources = os.path.join(DATA_DIR, 'sources_test.hkl')\n",
    "\n",
    "\n",
    "# kitti_test = KITTI(test_file, test_sources, nt)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU.')\n",
    "\n",
    "model = PredNet(R_channels, A_channels, output_mode='prediction').to(device)\n",
    "model.load_state_dict(torch.load('kitti_training.pt'))\n",
    "\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "inputs = dataiter.next()\n",
    "\n",
    "\n",
    "# print('images are', images)\n",
    "print('inputs shapes are', inputs.shape)\n",
    "\n",
    "inputs = Variable(inputs.to(device))\n",
    "origin = inputs.cpu()[:, nt-1]\n",
    "print('origin:')\n",
    "print(type(origin))\n",
    "print(origin.size())\n",
    "\n",
    "print('here')\n",
    "print(origin[0].size())\n",
    "\n",
    "o_np = origin[1].numpy()\n",
    "print('numpy shape', o_np.shape)\n",
    "print('numpy type', o_np.dtype)\n",
    "print(o_np)\n",
    "\n",
    "# let's see if we can move the stuff around \n",
    "channel_op = np.moveaxis(o_np, 0, -1)\n",
    "\n",
    "\n",
    "print('numpy shape 2', channel_op.shape)\n",
    "\n",
    "channel_op = (channel_op * 255).astype(np.uint8)\n",
    "# print(channel_op)\n",
    "print('chan op shape', channel_op.shape)\n",
    "\n",
    "# 1/0\n",
    "\n",
    "im2 = (o_np[0] * 255).astype(np.uint8)\n",
    "print(im2)\n",
    "print(im2.shape)\n",
    "\n",
    "# im = Image.fromarray((o_np * 255).astype(np.uint8))\n",
    "im = Image.fromarray(channel_op)\n",
    "\n",
    "\n",
    "\n",
    "# print('in save_image. im shape is', im.shape)\n",
    "im.save('origin_sanity.png')\n",
    "\n",
    "\n",
    "# 1/0\n",
    "\n",
    "\n",
    "# origin = torchvision.utils.make_grid(origin, nrow=4)\n",
    "# save_image(origin, 'origin.jpg')\n",
    "pred = model(inputs)\n",
    "pred = pred.data.cpu().byte()\n",
    "print('preds below:')\n",
    "print(type(pred))\n",
    "print(pred.size())\n",
    "\n",
    "\n",
    "# pred = torchvision.utils.make_grid(pred, nrow=4)\n",
    "p_np = pred[1].numpy()\n",
    "\n",
    "p_np = np.moveaxis(p_np, 0, -1)\n",
    "\n",
    "print('pred shape', p_np.shape)\n",
    "print('numpy type', p_np.dtype)\n",
    "print(p_np)\n",
    "\n",
    "# im3 = p_np\n",
    "\n",
    "im3 = (p_np * 255).astype(np.uint8)\n",
    "print(im3)\n",
    "print(im3.shape)\n",
    "# im = Image.fromarray((o_np * 255).astype(np.uint8))\n",
    "foo = Image.fromarray(im3)\n",
    "\n",
    "foo.save('pred.png')\n",
    "\n",
    "# save_image(pred, 'predicted.jpg')\n",
    "\n",
    "# print('um sanity check')\n",
    "\n",
    "\n",
    "\n",
    "2/0\n",
    "\n",
    "\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print('DEBUG:')\n",
    "    \n",
    "    #unpack inputs \n",
    "#     inputs = batch['pixel_values']\n",
    "#     print('inputs are', inputs)    \n",
    "    \n",
    "    # okay, so in the kitti code, each time step is an image \n",
    "    # so stack the stuff with blanks\n",
    "    # or img copies? \n",
    "    \n",
    "    # ...\n",
    "    \n",
    "#     inputs = inputs[:,None,:,:,:]\n",
    "#     print('inputs shape are', inputs.shape)\n",
    "\n",
    "#     inputs[1] = torch.Tensor([10 for _ in range(10)])\n",
    "    # add some sort of thing for this \n",
    "    # add dim \n",
    "    # then expand it to everything \n",
    "    \n",
    "#     print('inputs shape are', inputs.shape)\n",
    "\n",
    "    \n",
    "    # maybe see the jeff code for this?\n",
    "    # or some weird foundation thing\n",
    "    \n",
    "    inputs = inputs.permute(0, 1, 4, 2, 3) # batch x time_steps x channel x width x height\n",
    "    inputs = Variable(inputs.to(device))\n",
    "    origin = inputs.data.cpu().byte()[:, nt-1]\n",
    "    print('origin:')\n",
    "    print(type(origin))\n",
    "    print(origin.size())\n",
    "\n",
    "    print('predicted:')\n",
    "    pred = model(inputs)\n",
    "    pred = pred.data.cpu().byte()\n",
    "    print(type(pred))\n",
    "    print(pred.size())\n",
    "    origin = torchvision.utils.make_grid(origin, nrow=4)\n",
    "    pred = torchvision.utils.make_grid(pred, nrow=4)\n",
    "#     save_image(origin, 'origin.jpg')\n",
    "#     save_image(pred, 'predicted.jpg')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725d83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03585302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
